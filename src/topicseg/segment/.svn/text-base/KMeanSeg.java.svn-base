/*
 * Copyright (C) 2010 Cluster of Excellence, Univ. of Saarland
 * Minwoo Jeong (minwoo.j@gmail.com) is a main developer.
 * This file is part of "TopicSeg" package.
 * This software is provided under the terms of LGPL.
 */

package unisaar.topicseg.segment;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.Collections;
import java.util.Date;
import java.util.HashSet;
import java.util.List;
import java.util.Random;

import org.apache.commons.math.stat.clustering.*;
import org.apache.log4j.Logger;

import cern.jet.random.Uniform;
import cern.jet.random.engine.MersenneTwister;
import cern.jet.random.engine.RandomEngine;

import unisaar.topicseg.document.*;
import unisaar.topicseg.model.DirchletMultinomial;
import unisaar.topicseg.model.GammaCache;
import unisaar.topicseg.segment.SingleSeg.SegmentPoint;
import unisaar.topicseg.utils.Annealer;
import unisaar.topicseg.utils.Eval;
import unisaar.topicseg.utils.Option;

/**
 * unisaar.topicseg.algorithm::KMeanSeg.java
 * A Kmeans++ system that performs Kmeans++ algorithm to cluster the sentences. 
 *
 * @author minwoo
 */
public class KMeanSeg extends Segmenter {

	private transient Logger logger = Logger.getLogger(KMeanSeg.class);
	
	protected DirchletMultinomial dcm;
	protected Corpus mCorpus;

	int maxKMeansIter;
	boolean useVerbose;
	int initRandSeed;
	RandomEngine randomEngine;

	public KMeanSeg() {
		dcm = new DirchletMultinomial();
	}

	public void initialize(Option config) {
		
		this.initRandSeed = config.contains("rand.seed") ? config.getInteger("rand.seed") : 0;
		this.maxKMeansIter = config.contains("kmeans.maxIter") ? config.getInteger("kmeans.maxIter") : 1000;
		this.useVerbose = config.contains("print.verbose") ? config.getBoolean("print.verbose") : true;
		
        // initializing random engine
        if (initRandSeed > 0) 
	        randomEngine = new MersenneTwister(initRandSeed);
        else {
	        Date thedate = new Date();
	        randomEngine = new MersenneTwister(thedate);
        }
	    Uniform.staticSetRandomEngine(randomEngine);
	    
        if (config.contains("exp.log")) {
			try {
				Option.addFileLogger(logger, config.getString("exp.log"));
			}
			catch (IOException e) {
				e.printStackTrace();
			}
        }
	}
	
	@Override
	public void run(DocumentSet docset, double alpha, double beta) {
		int nRefCluster = 0;
		Annotation annotation = docset.getAnnotation();
		int[][] refs = annotation.toArray();
		for (int d = 0; d < docset.size(); d++) {
			Document doc = docset.get(d);
			int K = Eval.numSeg(refs[doc.getId()], 0, refs[doc.getId()].length-1) + 1;
			HashSet<Integer> localSegIds = new HashSet<Integer>();
			for (int i = 0; i < refs[doc.getId()].length; i++)
				if (refs[doc.getId()][i] < 0) localSegIds.add(refs[doc.getId()][i]);
			nRefCluster += localSegIds.size();
		}
		nRefCluster = annotation.size();
		
		// clustering
		kmeans(docset, alpha, nRefCluster);
	}

	@Override
	public void run(Corpus corpus, double alpha, double beta) {
    	long timer = System.currentTimeMillis(); // timer

		for (int s = 0; s < corpus.size(); s++) {
			DocumentSet docset = corpus.get(s);
			run(docset, alpha, beta);
		}
		
	    // print the reference and hypothesis
	    if (useVerbose) {
	        for (DocumentSet docSet : corpus) {
	        	Annotation annotation = docSet.getAnnotation();
	        	if (annotation.size() > 0) {
		        	int[][] refs = annotation.toArray();
		        	for (Document doc : docSet) {
		        		int[] hyp = doc.getTopicLabel();
		                StringBuffer sb = new StringBuffer();
		        		sb.append("[" + docSet.getId() + "," + doc.getId() + "] ");
		        		for (int x = 0; x < hyp.length; x++)
		        			sb.append(refs[doc.getId()][x] + ":" + hyp[x] + " ");
		        		logger.info(sb);
		        	}
	        	}
	        }
	    }

		print(corpus, timer);
	}
	
	private void print(Corpus corpus, long timer) {
	    // print out
		double[] evalMetrics = evaluate(corpus);
		double tookTime = ((long)System.currentTimeMillis() - timer);
		logger.info(String.format("SingleSeg || loglike=%e p=%.4f/r=%.4f/f1=%.4f vi=%.4f ri=%.4f wd=%.4f time=%.2f nSeg=%.2f",
				0.,
				evalMetrics[5], evalMetrics[6], evalMetrics[7],
				evalMetrics[8], evalMetrics[9], 
				evalMetrics[1], evalMetrics[2],
				tookTime / 1000,
				evalMetrics[4]
				));
	}
	
	public void kmeans(DocumentSet docSet, double prior, int nRefCluster) {
		int W = docSet.getAlphabet().size();
		double[][][] tfidf = docSet.getTFIDF();
		Collection<SegmentPoint> data = new ArrayList<SegmentPoint>();
		
		int maxK = -Integer.MAX_VALUE;
		
		for (int i = 0; i < docSet.size(); i++) {
			Document doc = docSet.get(i);
			int nSeg = doc.size();
	        int[] hiddentopic = new int[doc.size()];
	        doc.setTopicLabel(hiddentopic);
	        
			for (int j = 0; j < nSeg; j++) {
				double[] phi = new double[W];
				for (int x = 0; x < W; x++)
					phi[x] += tfidf[i][j][x];
				SegmentPoint point = new SegmentPoint(i, j, j, j+1, phi);
				data.add(point);
			}
			
			if (nSeg > maxK)
				maxK = nSeg;
		}
		
		int k = Math.min(nRefCluster, maxK);
		KMeansPlusPlusClusterer<SegmentPoint> clusterer = new KMeansPlusPlusClusterer<SegmentPoint>(new Random(initRandSeed));
		List<Cluster<SegmentPoint>> clusters = clusterer.cluster(data, k, 1000);
		int clusterId = 0;
		for (Cluster<SegmentPoint> clu : clusters) {
			List<SegmentPoint> points = clu.getPoints();
			for (SegmentPoint p : points) {
				Document doc = docSet.get(p.docId);
				for (int pos = p.start; pos < p.end; pos++)
					doc.setTopicLabel(pos, points.size() > 1 ? clusterId : -1 * clusterId);
				//System.out.println(p.docId + " " + p.segId + " " + p.start + " " + p.end);
			}
			//System.out.println();
			clusterId++;
		}
		
		for (Document doc : docSet)
			doc.resetTopicLabel();
		
	}
	
	public class SegmentPoint implements Clusterable<SegmentPoint> {
		
		public int docId;
		public int segId;
		public int start;
		public int end;
		private double[] prob;
		
		public SegmentPoint(int docId, int segId, int start, int end, double[] prob) {
			this.docId = docId;
			this.segId = segId;
			this.start = start;
			this.end = end;
			this.prob = prob;
		}
		
		public double[] getProb() {
			return prob;
		}
		
		public SegmentPoint centroidOf(Collection<SegmentPoint> points) {
			double[] centroid = new double[getProb().length];
			for (SegmentPoint p : points) {
				for (int i = 0; i < centroid.length; i++) {
					centroid[i] += p.getProb()[i];
				}
			}
			for (int i = 0; i < centroid.length; i++)
				centroid[i] /= points.size();
			
			return new SegmentPoint(-1, -1, -1, -1, centroid);
		}

		public double distanceFrom(SegmentPoint p) {
//			double v = (klDiv(p.getProb(), getProb()) + klDiv(getProb(), p.getProb())) / 2.0;
//			// crude heuristics
//			if (p.docId == docId) {
//				if (Math.abs(p.segId - segId) < 2) 
//					v *= 1.5;
//				else
//					v *= 5.0;
//			}
//			return v;
			double val1 = 0., val2 = 0., val3 = 0.;
			double[] v1 = getProb();
			double[] v2 = p.getProb();
			for (int i = 0; i < v1.length; i++) {
				val1 += v1[i] * v2[i];
				val2 += v1[i]; val3 += v2[i];
			}
			
			double dist = val1 > 0 ? 1.0 - val1 / (Math.sqrt(val2) * Math.sqrt(val3)) : 1.0;
			return dist;
			
		}
		
	}
	
    protected double computeLogprob(Document doc, int start, int end, double prior) {
        double ret = 0;
        int W = doc.getAlphabet().size(); 
        
        if (prior == 0) 
        	return -Double.MAX_VALUE;
        
        SparseWordVector wordVector = doc.getMergedWordVector(start, end);
        ret = dcm.logDCM(wordVector, W, prior);
        
        return ret;
    }
    
    protected double[] probLM(Document doc, int start, int end, double prior) {
        int W = doc.getAlphabet().size();
        SparseWordVector wordVector = doc.getMergedWordVector(start, end);
        
        
    	double S = prior * W;
    	double[] prob = new double[W];
    	Arrays.fill(prob, 0);
    	
        double N = wordVector.sum();
        int[] ids = wordVector.getIds();
    	double[] vals = wordVector.getVals();
    	
        for (int i = 0; i < ids.length; i++) {
        	//double v = gammaCache.logGamma(vals[i] + prior) - gammaCache.logGamma(N+S);
        	prob[ids[i]] = (vals[i] + prior) / (S + N); 
        	//prob[ids[i]] = Math.exp(v);
        }

       	return prob;
    }
    
    public static double klDiv(double distrib1[], double distrib2[]) {
        double total = 0;
        double alpha = .99;
        for (int i = 0; i < distrib1.length; i++){
            double q = alpha * distrib1[i] + (1.0-alpha) * distrib2[i];
            double r = distrib2[i];
            if (r != 0)
                total += r * (Math.log(r) - Math.log(q));
        }
        return total;
    }

	@Override
	public void initializeRandom(int initSeed) {
		initRandSeed = initSeed;		
	}
}
